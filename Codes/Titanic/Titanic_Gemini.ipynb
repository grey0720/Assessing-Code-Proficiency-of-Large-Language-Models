{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f826902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I try to submit Kaggle competition \" Titanic Disaster\" for getting score. \n",
    "Titanic data needs to several steps for preprocessing dataset and training classification model for prediction. \n",
    "First, i need to get a train and test datasets from kaggle. \n",
    "Make a python code which can connect train and test datasets from kaggle competition.\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'path/to/train.csv' and 'path/to/test.csv' with actual file paths\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "test_data = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4caf61e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Summary - Train Data:\n",
      "             Missing Values  Percentage Missing(%)\n",
      "PassengerId               0               0.000000\n",
      "Survived                  0               0.000000\n",
      "Pclass                    0               0.000000\n",
      "Name                      0               0.000000\n",
      "Sex                       0               0.000000\n",
      "Age                     177              19.865320\n",
      "SibSp                     0               0.000000\n",
      "Parch                     0               0.000000\n",
      "Ticket                    0               0.000000\n",
      "Fare                      0               0.000000\n",
      "Cabin                   687              77.104377\n",
      "Embarked                  2               0.224467\n",
      "\n",
      "Missing Values Summary - Test Data:\n",
      "             Missing Values  Percentage Missing(%)\n",
      "PassengerId               0               0.000000\n",
      "Pclass                    0               0.000000\n",
      "Name                      0               0.000000\n",
      "Sex                       0               0.000000\n",
      "Age                      86              20.574163\n",
      "SibSp                     0               0.000000\n",
      "Parch                     0               0.000000\n",
      "Ticket                    0               0.000000\n",
      "Fare                      1               0.239234\n",
      "Cabin                   327              78.229665\n",
      "Embarked                  0               0.000000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We got a datasets from kaggle competiton.\n",
    "Next, we need to check there are missing values in each features for handling them.\n",
    "Make a python code to find a missing values in each features of datasets.\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "def find_missing_values(data):\n",
    "    \"\"\"\n",
    "    This function takes a pandas DataFrame and returns a DataFrame \n",
    "    showing the number and percentage of missing values for each feature.\n",
    "    \"\"\"\n",
    "    # Calculate number of missing values per feature\n",
    "    missing_values = data.isnull().sum()\n",
    "\n",
    "    # Calculate total number of values per feature\n",
    "    total_values = data.shape[0]\n",
    "\n",
    "    # Calculate percentage of missing values\n",
    "    percent_missing = (missing_values / total_values) * 100\n",
    "\n",
    "    # Create a DataFrame to display results\n",
    "    missing_summary = pd.DataFrame({'Missing Values': missing_values, 'Percentage Missing(%)': percent_missing})\n",
    "\n",
    "    return missing_summary\n",
    "\n",
    "# Find missing values in train data\n",
    "train_missing_summary = find_missing_values(train_data.copy())\n",
    "print(\"Missing Values Summary - Train Data:\")\n",
    "print(train_missing_summary)\n",
    "\n",
    "# Find missing values in test data (optional)\n",
    "test_missing_summary = find_missing_values(test_data.copy())\n",
    "print(\"\\nMissing Values Summary - Test Data:\")\n",
    "print(test_missing_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9beada72",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using isnull() method, we found missing values in Age, Cabin, Embarked, Fare features.\n",
    "Cabin has the most numerous missing values in train/test data both.\n",
    "How can i handle missing values in Cabin feature?\n",
    "Find a way of handling missing values of Cabin features.\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "# Drop the Cabin feature (assuming separate DataFrames)\n",
    "train_data.drop('Cabin', axis=1, inplace=True)\n",
    "test_data.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69fd1870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dummy/anaconda3/envs/python/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 2. Imputation with Machine Learning\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\ndef impute_age_with_model(data):\\n  \"\"\"\\n  This function imputes missing values in the \\'Age\\' feature using a Random Forest \\n  regressor trained on other features.\\n\\n  **Note:** This is a basic example. You can experiment with different models and \\n  features for better accuracy.\\n  \"\"\"\\n  # Select features used for prediction (excluding Age)\\n  features = [\\'Pclass\\', \\'SibSp\\', \\'Parch\\', \\'Fare\\', ...]  # Choose relevant features\\n\\n  # Split data into features (X) and target (y)\\n  X_train = data[features].dropna()\\n  y_train = data[\\'Age\\'].dropna()\\n\\n  # Train a Random Forest Regressor model\\n  model = RandomForestRegressor()\\n  model.fit(X_train, y_train)\\n\\n  # Predict missing Age values using the trained model\\n  data.loc[data[\\'Age\\'].isna(), \\'Age\\'] = model.predict(data[features][data[\\'Age\\'].isna()])\\n  return data\\n\\n# Apply imputation on train and test data (assuming separate DataFrames)\\ntrain_data = impute_age_with_model(train_data.copy())\\ntest_data = impute_age_with_model(test_data.copy())\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Like feature of Cabin, Age feature in both dataset has many missing values.\n",
    "We need to handle these missing values, but age has one of the highest correlationship between \"Survived\".\n",
    "Estimating of missing values in Age feature require high accuracy.\n",
    "How we can handle that features in both dataset?\n",
    "Find ways with python codes to handle missing values in Age features with high accuracy of estimation.\n",
    "'''\n",
    "# 1. Imputation with Feature Engineering\n",
    "import pandas as pd\n",
    "\n",
    "def impute_age_with_features(data):\n",
    "    \"\"\"\n",
    "    This function imputes missing values in the 'Age' feature using title \n",
    "    and Pclass information.\n",
    "\n",
    "    **Note:** This is a basic example. You can explore more sophisticated techniques\n",
    "    like feature interaction or machine learning models for better accuracy.\n",
    "    \"\"\"\n",
    "    # Create title categories (consider Mr, Mrs, Miss, etc.)\n",
    "    data['Title'] = data['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "    \n",
    "  # Fill missing Age with median based on title and Pclass categories\n",
    "    for title in data['Title'].unique():\n",
    "        for pclass in data['Pclass'].unique():\n",
    "            age_median = data[(data['Title'] == title) & (data['Pclass'] == pclass)]['Age'].median()\n",
    "            data.loc[(data['Age'].isna()) & (data['Title'] == title) & (data['Pclass'] == pclass), 'Age'] = age_median\n",
    "    \n",
    "    # According to the developer's judgment, if missing values are not filled, replace them with the median of the entire \"Age\" feature.\n",
    "    age_median_all = data['Age'].median()\n",
    "    \n",
    "    # If missing values are not replaced, impute them with the median of the entire \"Age\" feature.\n",
    "    data.loc[data['Age'].isna(), 'Age'] = age_median_all\n",
    "  # Drop the created 'Title' feature (optional)\n",
    "    data.drop('Title', axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Apply imputation on train and test data (assuming separate DataFrames)\n",
    "train_data = impute_age_with_features(train_data.copy())\n",
    "test_data = impute_age_with_features(test_data.copy())\n",
    "\n",
    "# GPT와의 중복을 막기 위해 1번 방식 사용\n",
    "'''\n",
    "# 2. Imputation with Machine Learning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def impute_age_with_model(data):\n",
    "  \"\"\"\n",
    "  This function imputes missing values in the 'Age' feature using a Random Forest \n",
    "  regressor trained on other features.\n",
    "\n",
    "  **Note:** This is a basic example. You can experiment with different models and \n",
    "  features for better accuracy.\n",
    "  \"\"\"\n",
    "  # Select features used for prediction (excluding Age)\n",
    "  features = ['Pclass', 'SibSp', 'Parch', 'Fare', ...]  # Choose relevant features\n",
    "\n",
    "  # Split data into features (X) and target (y)\n",
    "  X_train = data[features].dropna()\n",
    "  y_train = data['Age'].dropna()\n",
    "\n",
    "  # Train a Random Forest Regressor model\n",
    "  model = RandomForestRegressor()\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Predict missing Age values using the trained model\n",
    "  data.loc[data['Age'].isna(), 'Age'] = model.predict(data[features][data['Age'].isna()])\n",
    "  return data\n",
    "\n",
    "# Apply imputation on train and test data (assuming separate DataFrames)\n",
    "train_data = impute_age_with_model(train_data.copy())\n",
    "test_data = impute_age_with_model(test_data.copy())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a0d812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          418 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(4)\n",
      "memory usage: 32.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f721b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embarked : S\n",
      "Fare : 35.63\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this step, features which have missing values are only Embarked in train dataset, and Fare in test dataset.\n",
    "These features have only 1 or 2 missing values in each feature.\n",
    "Find ways for implementing missing values of each feature.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Imputation with Mode\n",
    "'''\n",
    "def impute_with_mode(data, feature):\n",
    "    \"\"\"\n",
    "    This function imputes missing values in a specific feature with the mode.\n",
    "    \"\"\"\n",
    "    # Find the most frequent value\n",
    "    mode = data[feature].mode()[0]\n",
    "\n",
    "    # Replace missing values with the mode\n",
    "    data.loc[data[feature].isna(), feature] = mode\n",
    "    return data\n",
    "\n",
    "# Apply imputation on train data for Embarked\n",
    "train_data = impute_with_mode(train_data.copy(), 'Embarked')\n",
    "\n",
    "# Apply imputation on test data for Fare (assuming separate DataFrames)\n",
    "test_data = impute_with_mode(test_data.copy(), 'Fare')\n",
    "'''\n",
    "\n",
    "# 2. Imputation with Mean/Median (Optional)\n",
    "def impute_with_mode(data, feature):\n",
    "    \"\"\"\n",
    "    This function imputes missing values with the mode (categorical) or mean/median (numerical).\n",
    "    \"\"\"\n",
    "    if pd.api.types.is_numeric_dtype(data[feature]):\n",
    "    # Use mean for numerical features\n",
    "        imputation_value = round(data[feature].mean(),2)\n",
    "        print(f'{feature} :', imputation_value)\n",
    "    else:\n",
    "    # Use mode for categorical features\n",
    "        imputation_value = data[feature].mode()[0]\n",
    "        print(f'{feature} :', imputation_value)\n",
    "    # Replace missing values\n",
    "    data.loc[data[feature].isna(), feature] = imputation_value\n",
    "    return data\n",
    "\n",
    "# Apply imputation on train data for Embarked\n",
    "train_data = impute_with_mode(train_data.copy(), 'Embarked')\n",
    "\n",
    "# Apply imputation on test data for Fare (assuming separate DataFrames)\n",
    "test_data = impute_with_mode(test_data.copy(), 'Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb958aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          418 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         418 non-null    float64\n",
      " 9   Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(4)\n",
      "memory usage: 32.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3148ec7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 2. One-Hot Encoding\\nimport pandas as pd\\n\\ndef encode_sex_onehot(data):\\n  \"\"\"\\n  This function encodes the \\'Sex\\' feature using one-hot encoding.\\n  \"\"\"\\n  data = pd.get_dummies(data, columns=[\\'Sex\\'], drop_first=True)  # Avoids dummy trap\\n  return data\\n\\n# Apply encoding on train and test data (assuming separate DataFrames)\\ntrain_data = encode_sex_onehot(train_data.copy())\\ntest_data = encode_sex_onehot(test_data.copy())\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "On encoding features, \"Sex\" feature would be first step.\n",
    "Sex feature has two kind of values, \"male\" and \"female\"\n",
    "Those two object types should be converted to data types which can applies as training data for classifier.\n",
    "How can i convert that feature?\n",
    "'''\n",
    "# 1. Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_sex_label(data):\n",
    "    \"\"\"\n",
    "    This function encodes the 'Sex' feature using label encoding.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    data['Sex'] = le.fit_transform(data['Sex'])\n",
    "    return data\n",
    "\n",
    "# Apply encoding on train and test data (assuming separate DataFrames)\n",
    "train_data = encode_sex_label(train_data.copy())\n",
    "test_data = encode_sex_label(test_data.copy())\n",
    "\n",
    "# GPT와의 중복을 막기 위해 1번 선택\n",
    "'''\n",
    "# 2. One-Hot Encoding\n",
    "import pandas as pd\n",
    "\n",
    "def encode_sex_onehot(data):\n",
    "  \"\"\"\n",
    "  This function encodes the 'Sex' feature using one-hot encoding.\n",
    "  \"\"\"\n",
    "  data = pd.get_dummies(data, columns=['Sex'], drop_first=True)  # Avoids dummy trap\n",
    "  return data\n",
    "\n",
    "# Apply encoding on train and test data (assuming separate DataFrames)\n",
    "train_data = encode_sex_onehot(train_data.copy())\n",
    "test_data = encode_sex_onehot(test_data.copy())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e03839f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 2. One-Hot Encoding\\nimport pandas as pd\\n\\ndef encode_embarked_onehot(data):\\n    \"\"\"\\n    This function encodes the \\'Embarked\\' feature using one-hot encoding.\\n    \"\"\"\\n    data = pd.get_dummies(data, columns=[\\'Embarked\\'], drop_first=True)  # Avoids dummy trap\\n    return data\\n\\n# Apply encoding on train and test data (assuming separate DataFrames)\\ntrain_data = encode_embarked_onehot(train_data.copy())\\ntest_data = encode_embarked_onehot(test_data.copy())\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Next step, \"Embarked\" feature need to convert into data type which can uses for training dataset of classifier.\n",
    "Convert \"Embarked\" feature into data type allowed as training data of classifier.\n",
    "'''\n",
    "# 1. Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_embarked_label(data):\n",
    "    \"\"\"\n",
    "    This function encodes the 'Embarked' feature using label encoding.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    data['Embarked'] = le.fit_transform(data['Embarked'])\n",
    "    return data\n",
    "\n",
    "# Apply encoding on train and test data (assuming separate DataFrames)\n",
    "train_data = encode_embarked_label(train_data.copy())\n",
    "test_data = encode_embarked_label(test_data.copy())\n",
    "\n",
    "# GPT와의 차별화를 위해 1번 방식 선택\n",
    "'''\n",
    "# 2. One-Hot Encoding\n",
    "import pandas as pd\n",
    "\n",
    "def encode_embarked_onehot(data):\n",
    "    \"\"\"\n",
    "    This function encodes the 'Embarked' feature using one-hot encoding.\n",
    "    \"\"\"\n",
    "    data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)  # Avoids dummy trap\n",
    "    return data\n",
    "\n",
    "# Apply encoding on train and test data (assuming separate DataFrames)\n",
    "train_data = encode_embarked_onehot(train_data.copy())\n",
    "test_data = encode_embarked_onehot(test_data.copy())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c52e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    int64  \n",
      " 5   Age          891 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Embarked     891 non-null    int64  \n",
      "dtypes: float64(2), int64(7), object(2)\n",
      "memory usage: 76.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf18c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We finished up handling missing values and encoding all features in two datasets.\n",
    "But \"Name\" and \"Ticket\" features have still \"object\" data type, which can not accept as training dataset.\n",
    "How can we change those two features into data types which allowed as training data of classifier?\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Drop Name and Ticket features\n",
    "train_data.drop(['Name', 'Ticket'], axis=1, inplace=True)\n",
    "test_data.drop(['Name', 'Ticket'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dba42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Sex          891 non-null    int64  \n",
      " 4   Age          891 non-null    float64\n",
      " 5   SibSp        891 non-null    int64  \n",
      " 6   Parch        891 non-null    int64  \n",
      " 7   Fare         891 non-null    float64\n",
      " 8   Embarked     891 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 62.8 KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()\n",
    "# 모든 feature가 분류기에 사용할 수 있는 데이터타입이 된 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77df291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of steps about splitting data, training and prediction are same with developpers built.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = train_data['Survived']\n",
    "train_data.drop(['Survived'], axis = 1, inplace = True)\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(train_data, target, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b7c7d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8268156424581006"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, y_train)\n",
    "acc_random_forest = accuracy_score(y_test, random_forest.predict(X_test))\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d70f0b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7877094972067039"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "xgboost = xgb.XGBClassifier()\n",
    "xgboost.fit(X_train, y_train)\n",
    "acc_xgboost = accuracy_score(y_test, xgboost.predict(X_test))\n",
    "acc_xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c2e4b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7988826815642458"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_ensemble = VotingClassifier(estimators=[(\"Random Forest\",random_forest),\n",
    "                                               (\"XGBoost\",xgboost)],\n",
    "                                  voting = 'soft')\n",
    "voting_ensemble.fit(X_train, y_train)\n",
    "acc_voting_ensemble= accuracy_score(y_test, voting_ensemble.predict(X_test))\n",
    "acc_voting_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b3b791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./gemini_vt.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Save\n",
    "import joblib\n",
    "\n",
    "joblib.dump(random_forest, './gemini_rf.pkl')\n",
    "joblib.dump(xgboost, './gemini_xgb.pkl')\n",
    "joblib.dump(voting_ensemble, './gemini_vt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26a3776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = random_forest.predict(test_data)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_data['PassengerId'],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv('Gemini_randomForest.csv', index=False)\n",
    "# kaggle score : 0.7488\n",
    "\n",
    "Y_pred = xgboost.predict(test_data)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_data['PassengerId'],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv('Gemini_xgb.csv', index=False)\n",
    "# kaggle score : 0.7655\n",
    "\n",
    "Y_pred = voting_ensemble.predict(test_data)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_data['PassengerId'],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv('Gemini_vt.csv', index=False)\n",
    "# kaggle score : 0.7679\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "993e3ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived       1.000000\n",
      "Sex            0.543351\n",
      "Pclass         0.338481\n",
      "Fare           0.257307\n",
      "Embarked       0.167675\n",
      "Parch          0.081629\n",
      "Age            0.061320\n",
      "SibSp          0.035322\n",
      "PassengerId    0.005007\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# correlation\n",
    "corr_df = pd.concat([train_data, target], axis = 1)\n",
    "\n",
    "corr_test = corr_df.corr()\n",
    "corr_target = abs(corr_test['Survived'])\n",
    "print(corr_target.sort_values(ascending = False)[: 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62e0fe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex      0.168+/-  0.025\n",
      "Age      0.029+/-  0.015\n",
      "Pclass   0.027+/-  0.017\n",
      "Fare    -0.009+/-  0.016\n",
      "Parch   -0.009+/-  0.009\n",
      "SibSp   -0.012+/-  0.008\n",
      "Embarked-0.013+/-  0.009\n",
      "PassengerId-0.032+/-  0.013\n"
     ]
    }
   ],
   "source": [
    "# pfi - random forest\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "pfi = permutation_importance(random_forest, X_test, y_test,\n",
    "                            n_repeats=100, random_state=0)\n",
    "\n",
    "for i in pfi.importances_mean.argsort()[::-1]: #argsort = 인덱스 정렬\n",
    "    print(f\"{X_test.columns[i] : <8}\"\n",
    "          f\"{pfi.importances_mean[i] : .3f}\"    \n",
    "          f\"+/- {pfi.importances_std[i] : .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3502fffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex      0.157+/-  0.024\n",
      "Pclass   0.072+/-  0.021\n",
      "Age      0.022+/-  0.017\n",
      "SibSp   -0.003+/-  0.008\n",
      "Embarked-0.003+/-  0.008\n",
      "Parch   -0.006+/-  0.003\n",
      "Fare    -0.025+/-  0.017\n",
      "PassengerId-0.043+/-  0.017\n"
     ]
    }
   ],
   "source": [
    "# pfi - xgboost\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "pfi = permutation_importance(xgboost, X_test, y_test,\n",
    "                            n_repeats=100, random_state=0)\n",
    "\n",
    "for i in pfi.importances_mean.argsort()[::-1]: #argsort = 인덱스 정렬\n",
    "    print(f\"{X_test.columns[i] : <8}\"\n",
    "          f\"{pfi.importances_mean[i] : .3f}\"    \n",
    "          f\"+/- {pfi.importances_std[i] : .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5dc7bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex      0.165+/-  0.026\n",
      "Pclass   0.056+/-  0.020\n",
      "Age      0.019+/-  0.016\n",
      "Embarked-0.000+/-  0.007\n",
      "SibSp   -0.000+/-  0.007\n",
      "Parch   -0.010+/-  0.006\n",
      "Fare    -0.030+/-  0.016\n",
      "PassengerId-0.049+/-  0.013\n"
     ]
    }
   ],
   "source": [
    "# pfi - voting ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "pfi = permutation_importance(voting_ensemble, X_test, y_test,\n",
    "                            n_repeats=100, random_state=0)\n",
    "\n",
    "for i in pfi.importances_mean.argsort()[::-1]: #argsort = 인덱스 정렬\n",
    "    print(f\"{X_test.columns[i] : <8}\"\n",
    "          f\"{pfi.importances_mean[i] : .3f}\"    \n",
    "          f\"+/- {pfi.importances_std[i] : .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cebcbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "Best Random Forest Score: 0.8300600807643062\n",
      "Best XGBoost Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 100}\n",
      "Best XGBoost Score: 0.8286910272825765\n",
      "Best Voting Ensemble Parameters: {'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 300}\n",
      "Best Voting Ensemble Score: 0.8258642765685019\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 8, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 8],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Define your models\n",
    "rf_model = RandomForestClassifier()\n",
    "xgb_model = XGBClassifier()\n",
    "voting_model = VotingClassifier(estimators=[('rf', rf_model), ('xgb', xgb_model)], voting='hard')\n",
    "\n",
    "# Create GridSearchCV objects\n",
    "rf_cv = GridSearchCV(rf_model, param_grid=rf_param_grid)\n",
    "xgb_cv = GridSearchCV(xgb_model, param_grid=xgb_param_grid)\n",
    "voting_cv = GridSearchCV(voting_model, param_grid= {'rf__'+key: rf_param_grid[key] for key in rf_param_grid}, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV objects to your training data (X_train, y_train)\n",
    "rf_cv.fit(X_train, y_train)\n",
    "xgb_cv.fit(X_train, y_train)\n",
    "voting_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and scores for each model\n",
    "print(\"Best Random Forest Parameters:\", rf_cv.best_params_)\n",
    "print(\"Best Random Forest Score:\", rf_cv.best_score_)\n",
    "\n",
    "print(\"Best XGBoost Parameters:\", xgb_cv.best_params_)\n",
    "print(\"Best XGBoost Score:\", xgb_cv.best_score_)\n",
    "\n",
    "print(\"Best Voting Ensemble Parameters:\", voting_cv.best_params_)\n",
    "print(\"Best Voting Ensemble Score:\", voting_cv.best_score_)\n",
    "\n",
    "# Use the best models from GridSearchCV for prediction\n",
    "rf_best_model = rf_cv.best_estimator_\n",
    "xgb_best_model = xgb_cv.best_estimator_\n",
    "voting_best_model = voting_cv.best_estimator_\n",
    "\n",
    "# Make predictions on test data using the best models\n",
    "rf_predictions = rf_best_model.predict(X_test)\n",
    "xgb_predictions = xgb_best_model.predict(X_test)\n",
    "voting_predictions = voting_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6921410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "Best XGBoost Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 100}\n",
      "Best Voting Ensemble Parameters: {'rf__max_depth': 10, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Random Forest Parameters:\", rf_cv.best_params_)\n",
    "print(\"Best XGBoost Parameters:\", xgb_cv.best_params_)\n",
    "print(\"Best Voting Ensemble Parameters:\", voting_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eac33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "rf_hp = joblib.load('./gemini_rf_HP.pkl')\n",
    "xgb_hp = joblib.load('./gemini_xgb_HP.pkl')\n",
    "vt_hp = joblib.load('./gemini_vt_HP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b32d4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex      0.187+/-  0.025\n",
      "Age      0.043+/-  0.012\n",
      "Pclass   0.035+/-  0.016\n",
      "SibSp    0.003+/-  0.007\n",
      "Embarked 0.000+/-  0.007\n",
      "Parch   -0.006+/-  0.009\n",
      "PassengerId-0.008+/-  0.010\n",
      "Fare    -0.012+/-  0.011\n"
     ]
    }
   ],
   "source": [
    "# pfi - random forest\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "pfi = permutation_importance(rf_hp, X_test, y_test,\n",
    "                            n_repeats=100, random_state=0)\n",
    "\n",
    "for i in pfi.importances_mean.argsort()[::-1]: #argsort = 인덱스 정렬\n",
    "    print(f\"{X_test.columns[i] : <8}\"\n",
    "          f\"{pfi.importances_mean[i] : .3f}\"    \n",
    "          f\"+/- {pfi.importances_std[i] : .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12f0fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex      0.182+/-  0.026\n",
      "Pclass   0.074+/-  0.019\n",
      "Age      0.038+/-  0.015\n",
      "Parch    0.000+/-  0.000\n",
      "SibSp   -0.001+/-  0.006\n",
      "Fare    -0.003+/-  0.016\n",
      "Embarked-0.006+/-  0.008\n",
      "PassengerId-0.027+/-  0.014\n"
     ]
    }
   ],
   "source": [
    "# pfi - XGB\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "pfi = permutation_importance(xgb_hp, X_test, y_test,\n",
    "                            n_repeats=100, random_state=0)\n",
    "\n",
    "for i in pfi.importances_mean.argsort()[::-1]: #argsort = 인덱스 정렬\n",
    "    print(f\"{X_test.columns[i] : <8}\"\n",
    "          f\"{pfi.importances_mean[i] : .3f}\"    \n",
    "          f\"+/- {pfi.importances_std[i] : .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a440eee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex      0.177+/-  0.027\n",
      "Pclass   0.060+/-  0.020\n",
      "Age      0.042+/-  0.014\n",
      "Embarked 0.013+/-  0.008\n",
      "Parch   -0.002+/-  0.006\n",
      "Fare    -0.005+/-  0.014\n",
      "SibSp   -0.008+/-  0.008\n",
      "PassengerId-0.013+/-  0.013\n"
     ]
    }
   ],
   "source": [
    "# pfi - VT\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "pfi = permutation_importance(vt_hp, X_test, y_test,\n",
    "                            n_repeats=100, random_state=0)\n",
    "\n",
    "for i in pfi.importances_mean.argsort()[::-1]: #argsort = 인덱스 정렬\n",
    "    print(f\"{X_test.columns[i] : <8}\"\n",
    "          f\"{pfi.importances_mean[i] : .3f}\"    \n",
    "          f\"+/- {pfi.importances_std[i] : .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "402340e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./gemini_vt_HP.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Save - HP tuned\n",
    "import joblib\n",
    "\n",
    "joblib.dump(rf_best_model, './gemini_rf_HP.pkl')\n",
    "joblib.dump(xgb_best_model, './gemini_xgb_HP.pkl')\n",
    "joblib.dump(voting_best_model, './gemini_vt_HP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "997c3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = rf_best_model.predict(test_data)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_data['PassengerId'],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv('Gemini_randomForest_HPtuned.csv', index=False)\n",
    "# kaggle score : 0.7535\n",
    "\n",
    "Y_pred = xgb_best_model.predict(test_data)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_data['PassengerId'],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv('Gemini_xgb_HPtuned.csv', index=False)\n",
    "# kaggle score : 0.7583\n",
    "\n",
    "Y_pred = voting_best_model.predict(test_data)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_data['PassengerId'],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv('Gemini_vt_HPtuned.csv', index=False)\n",
    "# kaggle score : 0.7535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550792d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
