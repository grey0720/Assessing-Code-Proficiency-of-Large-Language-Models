# Assessing-Code-Proficiency-of-Large-Language-Models
This repository provides supplementary materials for the research investigating the coding capabilities of Large Language Models (LLMs) in machine learning (ML) workflows. The study explores whether LLMs can generate accurate code snippets, participate in ML pipeline tasks such as data preprocessing, algorithm selection, and hyperparameter tuning, and how their performance compares with human developers and automated ML frameworks. Through this comparative analysis, the findings aim to highlight the current role and future potential of LLMs in augmenting ML development processes.

The table below contains links to Notion pages that store all prompts and answers used by LLMs (ChatGPT/Gemini).

|Dataset     |Link   |
|:------------:|:--------------:|
|Titanic |http://tiny.cc/93xpzz |
|MNIST |http://tiny.cc/e3xpzz|
|Steel Defect|http://tiny.cc/h3xpzz|
